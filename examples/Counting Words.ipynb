{
 "metadata": {
  "name": "",
  "signature": "sha256:56dc69cf68c9c2490c4b2c8a6b22c5d0ac05725d486c4e74704aa84cac308c5a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Counting words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because what's a parallel computing demo without counting words?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some utilitiles for excluding commmon phrases and normalizing words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "non_word = re.compile(r'[\\W\\d]+', re.UNICODE)\n",
      "\n",
      "def normalize_word(word):\n",
      "    \"\"\"normalize a word\n",
      "    \n",
      "    simply strips non-word characters and case\n",
      "    \"\"\"\n",
      "    word = word.lower()\n",
      "    word = non_word.sub('', word)\n",
      "    return word\n",
      "\n",
      "common_words = {\n",
      "'the','of','and','in','to','a','is','it','that','which','as','on','by',\n",
      "'be','this','with','are','from','will','at','you','not','for','no','have',\n",
      "'i','or','if','his','its','they','but','their','one','all','he','when',\n",
      "'than','so','these','them','may','see','other','was','has','an','there',\n",
      "'more','we','footnote', 'who', 'had', 'been',  'she', 'do', 'what',\n",
      "'her', 'him', 'my', 'me', 'would', 'could', 'said', 'am', 'were', 'very',\n",
      "'your', 'did', 'not',\n",
      "}\n",
      "\n",
      "def yield_words(filename):\n",
      "    \"\"\"returns a generator of words in a file\"\"\"\n",
      "    import io\n",
      "    with io.open(filename, encoding='latin-1') as f:\n",
      "        for line in f:\n",
      "            for word in line.split():\n",
      "                word = normalize_word(word)\n",
      "                if word:\n",
      "                    yield word\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A function that reads a file, and returns a dictionary\n",
      "with string keys of phrases of `n` words,\n",
      "whose values"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngrams(filename, n=1):\n",
      "    \"\"\"compute ngram counts for the contents of a file\"\"\"\n",
      "    word_iterator = yield_words(filename)\n",
      "    counts = {}\n",
      "    def _count_gram(gram):\n",
      "        common = sum(word in common_words for word in gram)\n",
      "        if common > n / 2.0:\n",
      "            # don't count ngrams that are >= 50% common words\n",
      "            return\n",
      "        sgram = ' '.join(gram)\n",
      "        counts.setdefault(sgram, 0)\n",
      "        counts[sgram] += 1\n",
      "    \n",
      "    gram = []\n",
      "    \n",
      "    # get the first word\n",
      "    while len(gram) < n:\n",
      "        try:\n",
      "            word = next(word_iterator)\n",
      "            if not word:\n",
      "                continue\n",
      "        except StopIteration:\n",
      "            return counts\n",
      "        else:\n",
      "            gram.append(word)\n",
      "    \n",
      "    _count_gram(gram)\n",
      "\n",
      "    while True:\n",
      "        try:\n",
      "            word = next(word_iterator)\n",
      "        except StopIteration:\n",
      "            break\n",
      "        else:\n",
      "            gram.append(word)\n",
      "            gram.pop(0)\n",
      "            _count_gram(gram)\n",
      "    return counts\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile cathat.txt\n",
      "the cat in the hat is a cat whose hat is big."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngrams('cathat.txt', 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ngrams('cathat.txt', 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now fetch some interesting data from Project Gutenberg:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: \n",
      "    from urllib.request import urlretrieve # py3\n",
      "except ImportError:\n",
      "    from urllib import urlretrieve # py2\n",
      "\n",
      "davinci_url = \"http://www.gutenberg.org/cache/epub/5000/pg5000.txt\"\n",
      "\n",
      "if not os.path.exists('davinci.txt'):\n",
      "    # download from project gutenberg\n",
      "    print(\"Downloading Da Vinci's notebooks from Project Gutenberg\")\n",
      "    urlretrieve(davinci_url, 'davinci.txt')\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "def print_common(freqs, n=10):\n",
      "    \"\"\"Print the n most common keys by count.\"\"\"\n",
      "    words, counts = freqs.keys(), freqs.values()\n",
      "    items = zip(counts, words)\n",
      "    items.sort(reverse=True)\n",
      "    justify = 0\n",
      "    for (count, word) in items[:n]:\n",
      "        justify = max(justify, len(word))\n",
      "    \n",
      "    for (count, word) in items[:n]:\n",
      "        print(word.rjust(justify), count)\n",
      "    sys.stdout.flush()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run the serial version\n",
      "print(\"Serial word frequency count:\")\n",
      "%time counts = ngrams('davinci.txt', 1)\n",
      "print_common(counts, 10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And let's connect to a local cluster,\n",
      "and partition the text so we have one file per engine."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import parallel\n",
      "rc = parallel.Client()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split the davinci.txt into one file per engine:\n",
      "text = open('davinci.txt').read()\n",
      "lines = text.splitlines()\n",
      "nlines = len(lines)\n",
      "n = len(rc)\n",
      "\n",
      "block = nlines//n\n",
      "for i in range(n):\n",
      "    chunk = lines[i*block:(i+1)*(block)]\n",
      "    with open('davinci%i.txt'%i, 'w') as f:\n",
      "        f.write('\\n'.join(chunk))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "cwd = os.path.abspath(os.getcwd())\n",
      "fnames = [ os.path.join(cwd, 'davinci%i.txt'%i) for i in range(n)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eall = rc[:]\n",
      "eall.push(dict(\n",
      "    non_word=non_word,\n",
      "    yield_words=yield_words,\n",
      "    common_words=common_words,\n",
      "    normalize_word=normalize_word,\n",
      "))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exercise: parallel ngrams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Write a version of ngrams that runs in parallel,\n",
      "rejoining the results into a single count dict."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def ngrams_parallel(view, fnames, n=1):\n",
      "    \"\"\"Compute ngrams in parallel\n",
      "    \n",
      "    view - An IPython DirectView\n",
      "    fnames - The filenames containing the split data.\n",
      "    \"\"\"\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load ../soln/ngrams.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Parallel ngrams\")\n",
      "%time pcounts = ngrams_parallel(eall, fnames, 3)\n",
      "print_common(pcounts, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "A bit more data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Download some Project Gutenberg samples from ntlk (avoid rate-limiting on PG itself)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gutenberg_samples = 'http://nltk.github.com/nltk_data/packages/corpora/gutenberg.zip'\n",
      "if not os.path.isdir('gutenberg'):\n",
      "    if not os.path.exists('gutenberg.zip'):\n",
      "        urlretrieve(gutenberg_samples, 'gutenberg.zip')\n",
      "    !unzip gutenberg.zip\n",
      "\n",
      "import glob\n",
      "gutenberg_files = glob.glob(os.path.abspath(os.path.join('gutenberg', '*.txt')))\n",
      "# remove the bible, because it's too big relative to the rest\n",
      "gutenberg_files.remove(os.path.abspath(os.path.join('gutenberg', 'bible-kjv.txt')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls gutenberg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Parallel ngrams across several books\")\n",
      "%time pcounts = ngrams_parallel(eall, gutenberg_files, 3)\n",
      "print()\n",
      "print_common(pcounts, 10)\n",
      "pcounts = ngrams_parallel(eall, gutenberg_files, 4)\n",
      "print()\n",
      "print_common(pcounts, 10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}