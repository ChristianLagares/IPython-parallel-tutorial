{
 "metadata": {
  "name": "",
  "signature": "sha256:51f15912098e0f8bd1942f5cc37f47630801b0e9e54290a3cf87cb6cbc9da5a2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "non_word = re.compile(r'[\\W\\d]+', re.UNICODE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "common_words = {\n",
      "'the','of','and','in','to','a','is','it','that','which','as','on','by',\n",
      "'be','this','with','are','from','will','at','you','not','for','no','have',\n",
      "'i','or','if','his','its','they','but','their','one','all','he','when',\n",
      "'than','so','these','them','may','see','other','was','has','an','there',\n",
      "'more','we','footnote', 'who',\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def yield_words(filename):\n",
      "    import io\n",
      "    with io.open(filename, encoding='utf-8') as f:\n",
      "        for line in f:\n",
      "            for word in line.split():\n",
      "                yield word"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def normalize_word(word):\n",
      "    \"\"\"normalize a word\n",
      "    \n",
      "    simply strips non-word characters, and is case-insenstitive\n",
      "    \"\"\"\n",
      "    word = word.lower()\n",
      "    word = non_word.sub('', word)\n",
      "    return word\n",
      "\n",
      "def ngrams(filename, n):\n",
      "    word_iterator = yield_words(filename)\n",
      "    counts = {}\n",
      "    def _count_gram(gram):\n",
      "        common = sum(word in common_words for word in gram)\n",
      "        if common > n / 2.0:\n",
      "            # don't count ngrams that are >= 50% common words\n",
      "            return\n",
      "        sgram = ' '.join(gram)\n",
      "        counts.setdefault(sgram, 0)\n",
      "        counts[sgram] += 1\n",
      "    \n",
      "    gram = []\n",
      "    \n",
      "    # get the first word\n",
      "    while len(gram) < n:\n",
      "        try:\n",
      "            word = normalize_word(next(word_iterator))\n",
      "            if not word:\n",
      "                continue\n",
      "        except StopIteration:\n",
      "            return counts\n",
      "        else:\n",
      "            gram.append(word)\n",
      "    \n",
      "    _count_gram(gram)\n",
      "\n",
      "    while True:\n",
      "        try:\n",
      "            word = normalize_word(next(word_iterator))\n",
      "            if not word:\n",
      "                continue\n",
      "        except StopIteration:\n",
      "            break\n",
      "        else:\n",
      "            gram.append(word)\n",
      "            gram.pop(0)\n",
      "            _count_gram(gram)\n",
      "    return counts\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try: \n",
      "    from urllib.request import urlretrieve # py3\n",
      "except ImportError:\n",
      "    from urllib import urlretrieve # py2\n",
      "\n",
      "davinci_url = \"http://www.gutenberg.org/cache/epub/5000/pg5000.txt\"\n",
      "\n",
      "if not os.path.exists('davinci.txt'):\n",
      "    # download from project gutenberg\n",
      "    print(\"Downloading Da Vinci's notebooks from Project Gutenberg\")\n",
      "    urlretrieve(davinci_url, 'davinci.txt')\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloading Da Vinci's notebooks from Project Gutenberg\n"
       ]
      }
     ],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_common(freqs, n=10):\n",
      "    \"\"\"Print the n most common keys by count.\"\"\"\n",
      "    \n",
      "    words, counts = freqs.keys(), freqs.values()\n",
      "    items = zip(counts, words)\n",
      "    items.sort(reverse=True)\n",
      "    for (count, word) in items[:n]:\n",
      "        print(word, count)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Run the serial version\n",
      "print(\"Serial word frequency count:\")\n",
      "%time counts = ngrams('davinci.txt', 1)\n",
      "print_common(counts, 10)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Serial word frequency count:\n",
        "CPU times: user 1.55 s, sys: 17.2 ms, total: 1.57 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1.57 s\n",
        "light"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 852\n",
        "eye 591\n",
        "same 536\n",
        "shadow 507\n",
        "body 456\n",
        "between 446\n",
        "water 425\n",
        "seen 415\n",
        "leonardo 414\n",
        "into 402\n"
       ]
      }
     ],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split the davinci.txt into one file per engine:\n",
      "text = open('davinci.txt').read()\n",
      "lines = text.splitlines()\n",
      "nlines = len(lines)\n",
      "n = len(rc)\n",
      "block = nlines//n\n",
      "for i in range(n):\n",
      "    chunk = lines[i*block:(i+1)*(block)]\n",
      "    with open('davinci%i.txt'%i, 'w') as f:\n",
      "        f.write('\\n'.join(chunk))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import parallel\n",
      "rc = parallel.Client()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "cwd = os.path.abspath(os.getcwd())\n",
      "fnames = [ os.path.join(cwd, 'davinci%i.txt'%i) for i in range(n)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eall = rc[:]\n",
      "eall.push(dict(\n",
      "    common_words=common_words,\n",
      "    normalize_word=normalize_word,\n",
      "    non_word=non_word,\n",
      "    yield_words=yield_words,\n",
      "))\n",
      "\n",
      "def ngrams_parallel(view, fnames, n=1):\n",
      "    \"\"\"Compute ngrams in parallel\n",
      "    \n",
      "    view - An IPython DirectView\n",
      "    fnames - The filenames containing the split data.\n",
      "    \"\"\"\n",
      "    assert len(fnames) == len(view.targets)\n",
      "    view.scatter('fname', fnames, flatten=True)\n",
      "    ar = view.apply(ngrams, parallel.Reference('fname'), n)\n",
      "    freqs_list = ar.get()\n",
      "    counts = {}\n",
      "    for engine_count in ar:\n",
      "        for gram, count in engine_count.items():\n",
      "            if gram not in counts:\n",
      "                counts[gram] = 0\n",
      "            counts[gram] += count\n",
      "    return counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Parallel ngrams\")\n",
      "%time pcounts = ngrams_parallel(eall, fnames, 3)\n",
      "print_common(pcounts, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parallel ngrams\n",
        "CPU times: user 247 ms, sys: 30.8 ms, total: 278 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1.01 s\n",
        "light and shade"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 98\n",
        "the same way 44\n",
        "the luminous body 33\n",
        "between the eye 31\n",
        "seems to me 30\n",
        "the space between 29\n",
        "pen and ink 29\n",
        "leonardo da vinci 28\n",
        "the solar rays 27\n",
        "the right hand 27\n"
       ]
      }
     ],
     "prompt_number": 208
    }
   ],
   "metadata": {}
  }
 ]
}